# Hands-on 2: Analyzing Movie Script Dialogue Using Hadoop MapReduce

This hands-on activity is designed to extend your understanding of Hadoop MapReduce by analyzing a dataset of movie script dialogues. You will implement MapReduce jobs to extract and analyze:
1. The most frequently spoken words by characters.
2. Total dialogue length per character.
3. Unique words used by each character.

## Objectives

By completing this hands-on activity, students will:

1. **Enhance Text Processing Skills:** Learn to process and analyze text data to extract meaningful insights using Hadoop MapReduce.
2. **Develop Advanced MapReduce Jobs:** Implement more complex MapReduce jobs to perform tasks like finding the most frequently used words, calculating dialogue lengths, and extracting unique words by characters.
3. **Deploy and Run a Hadoop Cluster with Docker:** Learn how to deploy a Hadoop cluster using Docker and run MapReduce jobs on it.
4. **Submit and Manage Code Using GitHub:** Develop skills in managing code and submitting assignments via GitHub.

---

## Setup and Execution

### 1. **Fork the GitHub Repository**

- First, accept the GitHub Classroom invitation and fork the assignment repository to your own GitHub account.
- Once you’ve forked the repo, open the repository in **GitHub Codespaces** to begin working on the assignment.

---

### 2. **Start the Hadoop Cluster Using Docker Compose**

The repository contains a `docker-compose.yml` file that configures a Hadoop cluster. Run the following command in the GitHub Codespaces terminal to start the cluster:

```bash
docker-compose up -d
```

This command will spin up the necessary Hadoop components (ResourceManager, NodeManager, etc.) inside Docker containers.

---

### 3. **Build the Java Code with Maven**

After starting the cluster, use Maven to build the Java MapReduce code for the movie script analysis. In the terminal, execute the following command to compile and build the project:

```bash
mvn clean install
```

This command will generate a JAR file in the `target/` directory, which contains your MapReduce code.

---

### 4. **Prepare Input Data Files**

1. The input movie script dialogues dataset is located in the `input/` folder of the repository. Ensure that this file (`movie_dialogues.txt`) is present in the `input/` directory.

---

### 5. **Move the JAR and Input Files to the Docker Container**

#### **5.1 Move the JAR File to the Container**

Copy the built JAR file to the ResourceManager container. Run this command:

**Note**: Replace `<your-jar-file>` with the actual name of the JAR file generated by Maven.

```bash
docker cp target/<your-jar-file>.jar resourcemanager:/opt/hadoop-3.2.1/share/hadoop/mapreduce/
```

#### **5.2 Move the Input File to the Container**

Next, copy the movie script dialogues dataset to the ResourceManager container:

```bash
docker cp input/movie_dialogues.txt resourcemanager:/opt/hadoop-3.2.1/share/hadoop/mapreduce/
```

---

### 6. **Connect to the ResourceManager Container**

To run the Hadoop commands, you'll need to connect to the ResourceManager container:

```bash
docker exec -it resourcemanager /bin/bash
```

Once inside the container, navigate to the Hadoop directory where your files were copied:

```bash
cd /opt/hadoop-3.2.1/share/hadoop/mapreduce/
```

---

### 7. **Set Up HDFS for Input File**

To run the MapReduce job, the input file needs to be stored in Hadoop’s distributed file system (HDFS).

#### **7.1 Create Directories in HDFS**

Create a directory in HDFS for the input file:

```bash
hadoop fs -mkdir -p /input/movie_scripts
```

#### **7.2 Upload the Input File to HDFS**

Upload the movie script dialogues file to HDFS:

```bash
hadoop fs -put movie_dialogues.txt /input/movie_scripts/
```

---

### 8. **Execute the Movie Script Analysis MapReduce Jobs**

Now you are ready to run your MapReduce job for movie script analysis. This job will consist of three tasks:
1. Most Frequent Words by Character
2. Dialogue Length Analysis
3. Unique Words by Character

Run the job using the following command:

**Note**: Replace `<your-jar-file>` with the actual name of the JAR file generated by Maven.

```bash
hadoop jar <your-jar-file>.jar com.movie.script.analysis.MovieScriptAnalysis /input/movie_scripts/movie_dialogues.txt /output
```

This command will execute the MapReduce job with the movie script dialogues as the input and store the results in the `/output` directory in HDFS.

---

### 9. **View the Output of the MapReduce Job**

**Note**: The output will be stored in multiple directories (one for each task). You can view the output files using the following commands:

#### **9.1 List the Output Directories**

```bash
hadoop fs -ls /output
```

#### **9.2 View the Output Files for Each Task**

- **Task 1: Most Frequent Words by Character**
```bash
hadoop fs -cat /output/task1/part-r-00000
```

- **Task 2: Dialogue Length Analysis**
```bash
hadoop fs -cat /output/task2/part-r-00000
```

- **Task 3: Unique Words by Character**
```bash
hadoop fs -cat /output/task3/part-r-00000
```

These commands will display the results for each analysis task.

---

### 10. **Copy Output from HDFS to Local OS**

Once you have verified the results, copy the output from HDFS to your local file system.

#### **10.1 Copy Output from HDFS**

Use the following command to copy the output from HDFS to the Hadoop directory:

```bash
hadoop fs -get /output /opt/hadoop-3.2.1/share/hadoop/mapreduce/
```

#### **10.2 Copy Output from the Container to Your Local Machine**

Now, exit the ResourceManager container:

```bash
exit
```

Next, copy the output files from the Docker container to your GitHub Codespaces environment:

```bash
docker cp resourcemanager:/opt/hadoop-3.2.1/share/hadoop/mapreduce/output/ ./output/
```

---

### 11. **Submit Your Code and Output**

#### **11.1 Push Your Code and Output to GitHub**

Commit your changes, including the output from the MapReduce job, and push them to your GitHub repository:

```bash
git add .
git commit -m "Completed Movie Script Analysis Assignment"
git push origin main
```

#### **11.2 Submit the Assignment on GitHub Classroom**

Once you've pushed your code, go to GitHub Classroom and ensure your repository is submitted for the assignment. Make sure that the following are included:

1. The JAR file with your MapReduce job.
2. The input file (`movie_dialogues.txt`).
3. The output files from your MapReduce job.
4. The one-page report documenting the steps you followed, any challenges faced, and your observations.

---

### **Grading Criteria:**

1. **Correct Implementation of MapReduce Jobs:** The MapReduce jobs must correctly extract and analyze data as per the three tasks described.
2. **Proper Use of Hadoop and Docker:** Your solution must successfully deploy a Hadoop cluster using Docker and correctly manage input/output files using HDFS.
3. **Submission of Code and Output:** The correct output should be produced and submitted via GitHub, along with the code and a brief report.
4. **Report:** A clear one-page report summarizing your setup, challenges, and observations.

---

This concludes the instructions for the hands-on activity. If you encounter any issues, feel free to reach out during office hours or post your queries in the course discussion forum.

Good luck, and happy coding!

---